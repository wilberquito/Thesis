# Regularization thecniques[^1]

Why did we not figure out earlier that deep models are effective? There are many reasons, one reason is that deep models only really shine if you have enough data to train them. An other reason is now we now know better today how to train very big models using better regularization techniques.

A deep network that's just the right size for your data is very, very hard to optimize. In practice, we always try networks that are way to big for our data and then we try our best to preven them from overfitting.

## Early termination[^2]

The first way we prevent over fitting is by looking at the performance in the validation set and stopping in train as soon as we stop improving. This is the best way to prevent the network from over optimizing in training set.

[![Udacity](../_images/DNN-earlytermination.png)](https://classroom.udacity.com/courses/ud730/lessons/14e8621e-bc7f-4df6-a05a-df6a695c9791/concepts/ca710a33-f75e-4847-878f-1db635dad608)

## What is Regularization in ML?

Regularization is an approach to address over-fitting in ML. Overfitted model fails to generalize estimations on test data, regularization reduces the variance of the model.

Regularizing means applying artificial constrains on the network that implicitly reduce the number of free parameters. While not making it more difficult to optimize.

## How to introduce regularization in deep learning

bla bla bla

[^1]: [Regularization techniques for training deep neural networks](https://theaisummer.com/regularization/).

[^2]: [Early Termination](https://classroom.udacity.com/courses/ud730/lessons/14e8621e-bc7f-4df6-a05a-df6a695c9791/concepts/ca710a33-f75e-4847-878f-1db635dad608)

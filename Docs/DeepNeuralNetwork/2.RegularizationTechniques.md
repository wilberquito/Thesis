# Regularization thecniques[^1]

Why did we not figure out earlier that deep models are effective? There are many reasons, one reason is that deep models only really shine if you have enough data to train them. An other reason is now we now know better today how to train very big models using better regularization techniques.

A deep network that's just the right size for your data is very, very hard to optimize. In practice, we always try networks that are way to big for our data and then we try our best to preven them from overfitting.

## Early termination[^2]

The first way we prevent over fitting is by looking at the performance in the validation set and stopping in train as soon as we stop improving. This is the best way to prevent the network from over optimizing in training set.

[![Udacity](../_images/DNN-earlytermination.png)](https://classroom.udacity.com/courses/ud730/lessons/14e8621e-bc7f-4df6-a05a-df6a695c9791/concepts/ca710a33-f75e-4847-878f-1db635dad608)

## What is Regularization in ML?

Regularization is an approach to address over-fitting in ML. Overfitted model fails to generalize estimations on test data, regularization reduces the variance of the model.

Regularizing means applying artificial constrains on the network that implicitly reduce the number of free parameters. While not making it more difficult to optimize, also, an effective regularizer is one that makes a profitabletrade, reducing varianc signifcantly while not overly increasing the bias.

## How to introduce regularization in deep learning

The simplest way of introducing regularization is modifying the loss function. The most common family of approaches used are parameters norm penalties. Here we add a parameter norm penalty. Here for example, we add a parameter norm penalty $\Omega(\theta)$ to the loss function $J(\theta;X,y)$.

$$ J'(\theta;X,y) = J(\theta;X,y) + a \Omega(\theta) $$

Where $a$ is a hyperparameter that weights the contribution of the norm penalty, hence the effect of the regularization.

## L2 Regularization

Also known as weight decay or ridge regression, adds a norm penalty in the form of $\Omega(\theta) = \frac{1}{2}||w||^{2}_{2}$.

The loss function has been transformed to:

$$ J'(w;X,y) = J(w;X,y) + \frac{a}{2}||w||^{2}_{2} $$

If we compute the gradient we have:

[^1]: [Regularization techniques for training deep neural networks](https://theaisummer.com/regularization/).

[^2]: [Early Termination](https://classroom.udacity.com/courses/ud730/lessons/14e8621e-bc7f-4df6-a05a-df6a695c9791/concepts/ca710a33-f75e-4847-878f-1db635dad608)

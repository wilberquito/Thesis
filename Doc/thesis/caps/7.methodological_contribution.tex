\chapter{Methodological Contribution} \label{cap:contrib}


This chapter focuses on the analysis of critical aspects of the project. These
include the examination of essential elements such as data distribution,
machine learning techniques utilized for model training, and the consideration
of hyper-parameters during the training process. Additionally, we delve into
the reasons behind the selection of the base model for transfer learning and
the regularization methods employed. \\

Ultimately, we present a diagram that illustrates the training process of these
models and their final deployment.

\section{The Data}

In this section, we will elucidate the data source, the data analysis aimed at
highlighting the data distribution, the methods employed for data
transformation to facilitate regularization, our approach to dealing with the
challenge of a multiclass unbalanced data distribution. We finally explain the
way we create the necessary datasets from this data to effectively conduct our
work.

As previously mentioned, the data is acquired from the ISIC Archive,
specifically from the SIIM-ISIC Melanoma Classification competition available
on Kaggle \cite{ISICKaggle}. This challenge's dataset is a subset of the larger
ISIC Archive and includes data from 2019 and 2020. \\

For the thesis, we opted to use images with a resolution of 512x512 pixels,
despite the availability of higher resolutions such as 768x768 and 1024x1024.
The decision to select a lower resolution was purely technical, as higher
resolution images would require greater computational resources. \\

\subsection{Data Distribution}

For training the models, we utilized 8 classes selected from the original set,
as the remaining classes were considered residuals. Any samples that were not
categorized as one of the following classes were excluded from the training
process.

\begin{itemize}
  \item melanoma
  \item nevus
  \item BCC (Basal Cell Carcinoma)
  \item BKL (Benign lesions of the keratosis)
  \item AK (Actinic Keratosis)
  \item SCC (Squamous Cell Carcinoma)
  \item VASC (Vascular Lesions)
  \item DF (Dermatofibroma)
\end{itemize}

The filtered dataset comprises 31,265 distinct image samples, demonstrating an
imbalanced distribution across all classes. As evident from Figure
\ref{fig:hole-dataset-distribution}, we encounter the issue of dealing with an
imbalanced dataset, which poses challenges during the training process. Due to
the ease of targeting the majority class, models often exhibit a tendency to
overfit to the more frequent classes.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{imatges/methodological_contribution/hole-dataset-diagnosis.png}
  \caption[Data Distribution]{\textit{Data Distribution. }}
  {\label{fig:hole-dataset-distribution}}
\end{figure}

\subsection{Stratification}

Unbalanced datasets present a significant challenge in machine learning
problems. Various methods can be employed to address this issue, including
over-sampling, under-sampling, and more. In our approach, we decided to tackle
this problem by utilizing stratification in the train, validate, and test
datasets. This ensures that all datasets maintain the same distribution of
classes, even though they are unbalanced. \\

To address the issue of unbalanced distribution, we created a function that
made use of the {\tt stratify} parameter in the scikit-learn's {\tt
train\_test\_split} function, which is designed for such scenarios. By default,
this parameter is set to the labeled prediction, ensuring that the resulting
train, validate and test datasets maintain a proportional representation of the
different classes in the data.

\begin{Verbatim}[fontsize=\scriptsize]
def train_validate_split(df: pd.DataFrame,
                         random_state: int = 42,
                         validate_size: int = 0.25):
    """Split dataframe into random train and validate dataframe,
    it uses stratify thecnique because of the unbalanced dataset"""

    X_train, X_val = train_test_split(df,
                                      random_state=random_state,
                                      train_size=(1-validate_size),
                                      stratify=df['target'])
    X_train = pd.DataFrame(X_train)
    X_train.columns = df.columns
    X_val = pd.DataFrame(X_val)
    X_val.columns = df.columns

    return X_train, X_val
\end{Verbatim}

With reference to the code provided earlier, generating the training,
validation, and test datasets would entail the following steps:

\begin{Verbatim}[fontsize=\scriptsize]
train_df, validate_df = m_dataset.train_validate_split(df,
                                                       random_state=42,
                                                       validate_size=0.2)

validate_df, test_df = m_dataset.train_validate_split(validate_df,
                                                      random_state=42,
                                                      validate_size=0.5)
\end{Verbatim}

Note that the training dataset comprises 80\% of the filtered dataset, which
corresponds to a total of 25,012 samples. In contrast, both the validation and
test datasets are composed of 10\% of the filtered dataset, amounting to 3,126
samples each.

\subsection{Data Augmentation}

As it was explained in Chapter \ref{cap:prelim}, data augmentation is
a technique that creates more data by applying multiple transformations on an
image. The  main goal of data augmentation is to increase the volume, quality
and diversity of training data. \\

For the thesis, the python package {\tt albumentations} was used for this goal,
the package support a big variety of transformations, some of these
transformations are represented bellow.

\textbf{Horizontal and Vertical Flip} \\

An image flip means reversing the rows or columns of pixels in the case of a vertical or horizontal flip respectively.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{imatges/methodological_contribution/horizontal-flip.png}
  \caption[Horizontal Flip]{\textit{Horizontal Flip. \textbf{Left}: original image, \textbf{Right}: augmented image. }}
\end{figure}

\vspace{0.5cm}
\textbf{Gaussian Blur} \\

Blur the input image using a Gaussian filter with a random kernel size.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{imatges/methodological_contribution/gaussianblur.png}
  \caption[Gaussian Blur]{\textit{Gaussian Blur. \textbf{Left}: original image, \textbf{Right}: augmented image. }}
\end{figure}

\newpage

\textbf{Random Brightness Contrast} \\

Randomly change brightness and contrast of the input image.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{imatges/methodological_contribution/random-brigness-constrast.png}
  \caption[Random Brightness Contrast]{\textit{Random Brightness Contrast. \textbf{Left}: original image, \textbf{Right}: augmented image. }}
\end{figure}

As a result of applying the following data augmentation pipeline,

\begin{Verbatim}[fontsize=\scriptsize]
    transforms_train = A.Compose([
        A.Transpose(p=0.5),
        A.VerticalFlip(p=0.5),
        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(contrast_limit=0.2, p=0.75),
        A.OneOf([
            A.MotionBlur(blur_limit=5),
            A.MedianBlur(blur_limit=5),
            A.GaussianBlur(blur_limit=5),
            A.GaussNoise(var_limit=(5.0, 30.0)),
        ], p=0.7),
        A.OneOf([
            A.OpticalDistortion(distort_limit=1.0),
            A.GridDistortion(num_steps=5, distort_limit=1.),
            A.ElasticTransform(alpha=3),
        ], p=0.7),
        A.CLAHE(clip_limit=4.0, p=0.7),
        A.HueSaturationValue(hue_shift_limit=10,
                             sat_shift_limit=20,
                             val_shift_limit=10,
                             p=0.5),
        A.ShiftScaleRotate(shift_limit=0.1,
                           scale_limit=0.1,
                           rotate_limit=15,
                           border_mode=0,
                           p=0.85),
        A.Resize(image_size, image_size),
        A.CoarseDropout(max_holes=1,
                        max_height=int(image_size * 0.375),
                        max_width=int(image_size * 0.375),
                        p=0.7),
        A.Normalize(mean=mean, std=std)
    ])
\end{Verbatim}

The train dataset (Figure \ref{fig:sample-of-datasets}),

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{imatges/methodological_contribution/random-sample-of-isic.png}
  \caption[Random sample of images in the train dataset]{\textit{Random sample of images in the train dataset. }}
  {\label{fig:sample-of-datasets}}
\end{figure}

Is mapped into an augmented train dataset (Figure \ref{fig:aug-sample-of-datasets}).

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{imatges/methodological_contribution/random-sample-of-isic-augmented.png}
  \caption[Augmented random sample of images in the train dataset]{\textit{Augmented random sample of images in the train dataset. }}
  {\label{fig:aug-sample-of-datasets}}
\end{figure}

\newpage

\section{Modeling}

In this section, our objective is to elucidate the architecture of the models
utilized in the thesis, outline the training approaches employed for each
model, and introduce certain functionalities integrated into the training loop
to evaluate the models' performance effectively.

\subsection{ResNet-18}

We utilized the pre-trained weights of the ResNet-18 as the base model
for transfer learning. As described in Chapter \ref{cap:prelim}, this
model achieves an accuracy of approximately 69.76\% on the ImageNet
dataset, which may not be as high as some other models. However, due to its
relatively small number of trainable parameters (11.7 million), we decided to
select this model over other available options. \\

In ResNet architecture (Figure \ref{fig:resnet-18-arch}), each layer of the
network contains a residual block (BasicBlock).

\begin{figure}[H]
  \begin{adjustbox}{width=\textwidth, trim={0.2cm 0pt 1.5cm 0pt}, clip}
    \centering
    \includegraphics[width=\textwidth]{imatges/methodological_contribution/residual-blocks.png}
  \end{adjustbox}
  \caption[Resume of the ResNet-18 Architecture]{\textit{Resume of the
  ResNet-18 Architecture. }}
  {\label{fig:resnet-18-arch}}
\end{figure}

A residual block consists of multiple convolutional layers followed by batch
normalization and activation functions (\textit{Figure
\ref{fig:resnet-18-residual-block}}).

\newpage

\begin{figure}[H]
  %\begin{adjustbox}{width=\textwidth, trim={0.2cm 0pt 1.5cm 0pt}, clip}
  \centering
  \includegraphics[width=0.4\textwidth]{imatges/methodological_contribution/basic-block.png}
  %\end{adjustbox}
  \caption[ResNet18 Residual Block]{\textit{ResNet18 Residual Block. }}
  {\label{fig:resnet-18-residual-block}}
\end{figure}

The input to a residual block is added to its output through a skip connection,
which directly connects the input to the output Figure
\ref{fig:skip-connection}). This allows the network to learn the residual
information, or the difference between the input and the desired output, making
it easier for the network to learn the mapping.

\begin{figure}[H]
  %\begin{adjustbox}{width=\textwidth, trim={0.2cm 0pt 1.5cm 0pt}, clip}
  \centering
  \includegraphics[width=\textwidth]{imatges/methodological_contribution/skip-connections.png}
  %\end{adjustbox}
  \caption[ResNet Skip Connection]{\textit{ResNet Skip Connection. Illustration by medium}}
  {\label{fig:skip-connection}}
\end{figure}

We made use of the PyTorch API to use the pre-trained weight of ResNet-18 as follow:

\begin{Verbatim}[fontsize=\scriptsize]
from torchvision.models import (resnet18,
                                ResNet18_Weights)
net = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
\end{Verbatim}

\subsection{Proposed Models}

As explained in the section before, the base model is the ResNet-18. This
model classify between 1000 classes. But our goal is to classify between 8
different classes. \\

The initial strategy to address this problem involved encapsulating the ResNet
within a "Module" class, following the PyTorch framework. This approach enables
the weights within the class to undergo back-propagation, allowing them to be
trained effectively.

\begin{Verbatim}[fontsize=\scriptsize]
class ResNet18_Melanoma(nn.Module):

    def __init__(self, out_features: int):
        super(ResNet18_Melanoma, self).__init__()

        self.net = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
        in_features = self.net.fc.in_features
        self.net.fc = nn.Identity()
        self.myfc = nn.Linear(in_features, out_features)

    def transfer(self, x: torch.Tensor):
        return self.net(x)

    def forward(self, x: torch.Tensor):
        x = self.transfer(x).squeeze(-1).squeeze(-1)
        x = self.myfc(x)
        return x
\end{Verbatim}

As shown above, we employ the weights of the trained ResNet-18 model and save
it as an instance of the \texttt{ResNet18\_Melanoma} class (Figure
\ref{fig:resnet-18-melanoma-arch}). Next, we access this instance of ResNet-18
and modify its fully connected layer by replacing it with an Identity layer,
which effectively performs no operations. Subsequently, we instantiate a new
fully connected layer with the desired output size, which in this case is 8,
representing the number of classes.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{imatges/methodological_contribution/ResNet18_Melanoma.png}
  \caption[ResNet18\_Melanoma Architecture]{\textit{ResNet18\_Melanoma Architecture. }}
  {\label{fig:resnet-18-melanoma-arch}}
\end{figure}

During the model training process, the {\tt forward} function is triggered with
images represented as a tensor. It first performs transfer learning using the
base model through the transfer function. Then, it utilizes the output of the
transfer process to train the new fully connected layer. \\

In the evaluation and analyse stage of the different training approaches of the
{\tt ResNet18\_Melanoma} we observed significant over-fitting. To address this
issue, we propose the {\tt ResNet18\_Dropout\_Melanoma} model, which follows
the same behavior as the previous model but includes a regularization layer
that utilizes the average of 5 dropout layers after the transfer learning
process. This regularization layer helps to mitigate the over-fitting problem
found in the previous model.

\begin{Verbatim}[fontsize=\scriptsize]
class ResNet18_Dropout_Melanoma(nn.Module):
    """This model is based on ResNet18 but uses the
    average of the result of a list of dropout layers
    to avoid overfitting"""

    def __init__(self, out_features: int):
        super(ResNet18_Dropout_Melanoma, self).__init__()

        self.net = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
        in_features = self.net.fc.in_features
        self.net.fc = nn.Identity()
        self.myfc = nn.Linear(in_features, out_features)
        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])

    def transfer(self, x: torch.Tensor):
        return self.net(x)

    def forward(self, x: torch.Tensor):
        x = self.transfer(x).squeeze(-1).squeeze(-1)

        for i, dropout in enumerate(self.dropouts):
            if i == 0:
                out = self.myfc(dropout(x))
            else:
                out += self.myfc(dropout(x))

        out /= len(self.dropouts)
        return out
\end{Verbatim}

The term "recursive" used in the context of the architecture for
\texttt{ResNet18\_Dropout\_Melanoma} (Figure
\ref{fig:resnet-18-dropout-melanoma-arch}) indicates that the layers have been
"forwarded" multiple times instead of being constructed multiple times.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{imatges/methodological_contribution/ResNet18_Dropout_Melanoma.png}
  \caption[ResNet18\_Dropout\_Melanoma
  Architecture]{\textit{ResNet18\_Dropout\_Melanoma Architecture.
  }}
  {\label{fig:resnet-18-dropout-melanoma-arch}}
\end{figure}

You can instantiate any of these models with the necessary {\tt out\_features}
by following the code snippet below:

\begin{Verbatim}[fontsize=\scriptsize]
out_features = len(mapping)
model = m_models.ResNet18_Dropout_Melanoma(out_features)
\end{Verbatim}

Note that the variable {\tt out\_features} should be defined before doing any
instance.

\subsection{Train-Validate Loop}

\begin{Verbatim}[fontsize=\scriptsize]
def train_model(model: nn.Module,
                mel_idx: int,
                about_data: Dict,
                device: torch.device,
                criterion: nn.Module,
                optimizer: torch.optim.Optimizer,
                scheduler: torch.optim.lr_scheduler.LRScheduler = None,
                num_epochs: int = 25,
                patience: int = 5,
                writter: Writter = None,
                val_times: int = 1):
. . .
\end{Verbatim}

Among other features, the train validate loop supports:

\begin{itemize}
  \item Train the model using the training dataset.
  \item Validate the model using the validation dataset.
  \item Modify the optimizer's behavior using a scheduler.
  \item Implement an early stop mechanism by defining the {\tt patience} argument.
  \item Save the weights of the trained model and evaluate metrics using a higher-order function.
  \item Apply test-time augmentation during the validation phase, repeating it a certain number of times using the {\tt val\_times} argument.
\end{itemize}

\subsection{Writer Callback}

PyTorch does not provide built-in wrappers to automatically save model
performance during the training and validation phases. However, it is essential
to have information on how well a model performs after each epoch in order to
compare different training approaches. \\

In order to tackle this concern, we've incorporated a feature that saves the
model's weights at the end of each epoch. We also keep a record of the metrics
from both the training and validation stages in a CSV file. Furthermore, we
monitor the training procedure by sending the most recent metric updates using
the Weights \& Biases service. This approach enables us to effectively monitor
and analyze the training progress in real-time.

\begin{Verbatim}[fontsize=\scriptsize]
def model_writter(model_name: str):
    """Saves the pythorch trainned model and generates
    a log file in csv format with the current trainning
    and validation phases. It finally send the last log
    to wand service."""

    def writter(point: Dict):
        # Save checkpoint
        m_checkpoint.save_checkpoint(point,
                                     model_name + '.pth.tar')

        # Logging the trainning
        log_filename = model_name + '.csv'
        stats = point['stats']
        pd.DataFrame(stats).to_csv(log_filename)

        # wand logging
        wandb.log({
            "train_acc": stats["train_acc"][-1],
            "train_loss": stats["train_loss"][-1],
            "train_ovr": stats["train_ovr"][-1],
            "val_acc": stats["val_acc"][-1],
            "val_loss": stats["val_loss"][-1],
            "val_ovr": stats["val_ovr"][-1],
        })

    return writter
\end{Verbatim}

\newpage


\subsection{Training Models}

To facilitate understanding of the training descitions, we include a name
mapping in Table \ref{table:mapping-names}.

\begin{table}[H]
  \centering
  \begin{tabular}{cc}
    \toprule

    \multicolumn{2}{c}{\textbf{Name Mapping}} \\
    \midrule
    ResNet18\_V0 & M0 \\
    ResNet18\_V1 & M1 \\
    ResNet18\_V2 & M2 \\
    ResNet18\_V3 & M3 \\
    ResNet18R\_V0  & M4 \\
    ResNet18R\_V1  & M5 \\
    ResNet18R\_V2 & M6 \\
    ResNet18R\_V3  & M7 \\
    ResNet18\_Melanoma & R18M \\
    ResNet18\_Dropout\_Melanoma & R18DM \\
    Cosine Annealing Warm Restarts & CAWR \\
    Cosine Annealing Learning Rate & CALR \\
    Step Learning Rate & SLR \\
    Tesla T4, 16GB (GPU) & TT4 \\
    Nvidia A100, 80GB (GPU) & NA100 \\
    \bottomrule
  \end{tabular}
  \caption[Name Mapping]
  {\textit{Name Mapping. Simple visualization for next tables.
  }}
  {\label{table:mapping-names}}
\end{table}

We conducted training for eight models, employing various training approaches,
all of which relied on transfer learning with ResNet-18 pretrained weights.
These models utilized the Stochastic Gradient Descent optimizer with a base
learning rate of 0.001 and employed Cross-entropy loss as their loss function.
Descition that are not shared regarding the training process, including the
training environment and hyperparameters can be found in
Table \ref{table:trained-models-information}.

\newpage

\begin{landscape}

  \begin{table}
    \centering
    \begin{tabular}{lcccccccc}
      \toprule
      & \textbf{M0} & \textbf{M1} & \textbf{M2} & \textbf{M3} & \textbf{M4} & \textbf{M5} & \textbf{M6} & \textbf{M7} \\
      \midrule
      Model Architecture & R18M & R18M & R18M & R18M & R18DM & R18DM & R18DM & R18DM \\
      Epochs & 20 & 20 & 20 & 20 & 40 & 40 & 40 & 40 \\
      Batch Size & 400 & 400 & 400 & 400 & 1024 & 1024 & 1024 & 1024 \\
      Scheduler & & SLR & CALR & CAWR &  & SLR & CALR & CAWR  \\
      Data Augmentation & No & No & No & No  & Yes & Yes & Yes & Yes \\
      Dropout Regularization & No & No & No & No  & Yes & Yes & Yes & Yes \\
      GPU & TT4 & TT4 & TT4 & TT4 & NA100 & NA100 & NA100 & NA100 \\
      Training Time & 1h 45m & 1h 22m & 1h 43m & 1h 38m & 1d 7h 30m & 1d 7h 4m & 1d 7h 1m & 1d 12h 55m \\ \bottomrule
    \end{tabular}
    \caption[Training Information For Each Model.]
    {\textit{Training Information For Each Model. Empty spaces represent non-use of that feature.
    }}
    {\label{table:trained-models-information}}
  \end{table}
\end{landscape}

\newpage

\section{Evaluaton and Testing}

In this section, we will discuss the evaluation and testing mechanisms employed
to assess the performance of the models during both the training and testing
phases. In this section we also elaborate on the selection criteria for the
metric used to gauge the models' performance.

\subsection{The AUC Metric}

In the thesis, the main metric utilized to assess the model's performance is
the AUC (Area Under the Curve). Although AUC is typically employed for binary
problems, it is worth noting that the thesis deals with a multiclass dataset,
and the models are trained using the Cross-entropy Loss function, specifically
designed for handling multiclass problems. \\

Indeed, the models are trained to address an unbalanced multiclass problem.
However, during the evaluation phase, instead of employing a metric that
directly assesses the model's performance in the multiclass context, AUC is
used. This involves treating labels that are not related to melanoma as the
negative class and melanoma as the positive class. Consequently, the problem is
transformed from a multiclass problem to a binary problem, enabling the use of
AUC as a suitable metric to evaluate the models.

\subsection{Test-Time Augmentation}

Test-time augmentation (TTA) serves as an ensemble mechanism that proves
beneficial not only for making inferences on the test set but also for
inferring the validation set. Since the validation set doesn't undergo any data
augmentation during the prediction phase, we apply multiple dummy
transformations to it and average the resulting predictions to obtain the final
prediction. This technique significantly enhances the model's robustness and
reliability when making predictions on previously unseen data. \\

When applying TTA for prediction, we generate multiple predictions
by employing various transformations, and then we average these predictions to
arrive at the final outcome.

\begin{Verbatim}[fontsize=\scriptsize]
@torch.inference_mode()
def test_time_augmentation(model: torch.nn,
                           inputs: torch.Tensor,
                           val_times: int):
    """Applies time test time agumentation to a
    set of tensor images an returns the logits"""

    model.eval()

    logits = []
    for n in range(val_times):
        augmented_img = test_time_transform(inputs, n)
        augmented_img = torch.unsqueeze(augmented_img, 0)
        outputs = model(test_time_transform(inputs, n))
        logits.append(outputs)

    stacked_logits = torch.stack(logits)
    stacked_logits = torch.mean(stacked_logits, dim=0)

    return stacked_logits


def test_time_transform(img: torch.Tensor, n: int):
    """Given a tensor it applies dummy
    transformation on de n value"""

    if n >= 4:
        return img.transpose(2, 3)
    if n % 4 == 0:
        return img
    elif n % 4 == 1:
        return img.flip(2)
    elif n % 4 == 2:
        return img.flip(3)
    elif n % 4 == 3:
        return img.flip(2).flip(3)
\end{Verbatim}

\section{Pipeline}

In this section, we present the pipeline that was employed (Figure
\ref{fig:cad-infrastructure-training-system}), starting from data acquisition,
through the training process and evaluation, and finally to serving the trained
models for inference. \\

The first stage involves cleaning and splitting the initial dataset into
smaller datasets. This step ensures that the data is organized and ready for
further processing. \\

The second stage focuses on training and validating the models using the
training and validation datasets. During this stage, the system reads images
and applies data augmentation techniques to train images and Test Time
Augmentation (TTA) to validation images. These techniques enhance the model's
performance by introducing variations in the data and improving its
generalization ability. \\

The third stage involves analyzing the training results obtained from different
training approaches. In this section, we evaluate and analyze the model's
performance by comparing the predicted results against the test dataset. This
step helps us understand how well the models are learning and performing on
unseen data. \\

The last stage revolves around exposing the trained models through an API's
container image. This container image allows for easy deployment and
integration of the models into other systems or applications, providing a
convenient way to utilize the trained models for various tasks. \\


\begin{figure}[H]
  %\begin{adjustbox}{width=\textwidth, trim={0.2cm 0pt 1.5cm 0pt}, clip}
  \centering
  \includegraphics[width=\textwidth]{imatges/methodological_contribution/Pipeline.drawio.png}
  %\end{adjustbox}
  \caption[CAD Infrastructure Pipeline]{\textit{CAD Infrastructure Pipeline. }}
  {\label{fig:cad-infrastructure-training-system}}
\end{figure}

# Rectified linear units (ReLU)

ReLUs are the simplest non linear function you can think of. They are linear if $x \gt 0$ and 0 every where else. ReLUs has nice derivative as well.

![Udacity](../Img/DeepNeuralNetwork/ReLU.png)

